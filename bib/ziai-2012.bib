@comment{{This file has been generated by bib2bib 1.98}}

@comment{{Command line: /usr/bin/bib2bib -oc ziai-citations-2012 -ob ziai-2012.bib -c 'author : "Ziai"' -c year=2012 /home/rziai/work/icall/resources/bibs/icall.bib}}

@inproceedings{Ziai.Ott.ea-12a,
  author = {Ramon Ziai and Niels Ott and Detmar Meurers},
  title = {Short Answer Assessment: Establishing Links Between Research Strands},
  booktitle = {Proceedings of the 7th Workshop on Innovative Use of {NLP} for {B}uilding
	{E}ducational {A}pplications (BEA-7) at {NAACL-HLT} 2012},
  year = {2012},
  pages = {190--200},
  address = {Montreal},
  abstract = {A number of different research subfields are concerned with the automatic
	assessment of student answers to comprehension questions, from language
	learning contexts to computer science exams. They share the need
	to evaluate free-text answers but differ in task setting and grading/evaluation
	criteria, among others. This paper has the intention of fostering
	synergy between the different research strands. It discusses the
	different research strands, details the crucial differences, and
	explores under which circumstances systems can be compared given
	publicly available data. To that end, we present results with the
	CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the
	dataset published by Mohler et al. (2011) and outline what was necessary
	to perform this comparison. We conclude with a general discussion
	on comparability and evaluation of short answer assessment systems.},
  crossref = {bea-12},
  keywords = {rte recognizing textual entailment meaning comparison comic cosec
	automatic grading scoring weblas carmeltc c-rater oxford content
	assessment module cam bachman rose leacock chodorow mitchell pulman
	sukkarieh perez makatchev vanlehn bailey evaluation kappa gold standard
	kappa correlation},
  optpublisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/W12-2022.pdf}
}

@incollection{Ott.Ziai.Meurers-12,
  author = {Ott, Niels and Ziai, Ramon and Meurers, Detmar},
  title = {Creation and Analysis of a Reading Comprehension Exercise Corpus:
	Towards Evaluating Meaning in Context},
  booktitle = {Multilingual Corpora and Multilingual Corpus Analysis},
  publisher = {Benjamins},
  year = {2012},
  editor = {Thomas Schmidt and Kai W\"{o}rner},
  series = {Hamburg Studies in Multilingualism (HSM)},
  pages = {47--69},
  address = {Amsterdam},
  url = {http://purl.org/dm/papers/ott-ziai-meurers-12.html}
}

@misc{Ziai.Ott.ea-12,
  author = {Ramon Ziai and Niels Ott and Detmar Meurers},
  title = {Evaluating Answers to Reading Comprehension Questions in Context},
  howpublished = {Poster at the 2012 Annual Conference of {Deutsche} {Gesellschaft}
	f\"ur {Sprachwissenschaft} ({DGfS})},
  month = {3},
  year = {2012},
  address = {Frankfurt am Main, Germany},
  file = {:Ziai.Ott.ea-12-poster.pdf:PDF;Ziai.Ott.ea-12.pdf:Ziai.Ott.ea-12.pdf:PDF},
  owner = {nott},
  timestamp = {2012.03.12},
  url = {http://drni.de/zap/ziai-ott-ea-12-poster}
}

@inproceedings{Ziai.Ott.ea-12a-no-crossref,
  author = {Ramon Ziai and Niels Ott and Detmar Meurers},
  title = {Short Answer Assessment: Establishing Links Between Research Strands},
  booktitle = {Proceedings of the 7th Workshop on Innovative Use of {NLP} for {B}uilding
	{E}ducational {A}pplications (BEA-7) at {NAACL-HLT} 2012},
  year = {2012},
  pages = {190--200},
  address = {Montreal},
  month = {June},
  publisher = {ACL},
  longpublisher = {Association for Computational Linguistics},
  abstract = {A number of different research subfields are concerned with the automatic
	assessment of student answers to comprehension questions, from language
	learning contexts to computer science exams. They share the need
	to evaluate free-text answers but differ in task setting and grading/evaluation
	criteria, among others. This paper has the intention of fostering
	synergy between the different research strands. It discusses the
	different research strands, details the crucial differences, and
	explores under which circumstances systems can be compared given
	publicly available data. To that end, we present results with the
	CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the
	dataset published by Mohler et al. (2011) and outline what was necessary
	to perform this comparison. We conclude with a general discussion
	on comparability and evaluation of short answer assessment systems.},
  keywords = {rte recognizing textual entailment meaning comparison comic cosec
	automatic grading scoring weblas carmeltc c-rater oxford content
	assessment module cam bachman rose leacock chodorow mitchell pulman
	sukkarieh perez makatchev vanlehn bailey evaluation kappa gold standard
	kappa correlation},
  opteditor = {Joel Tetreault and Jill Burstein and Claudial Leacock},
  optpublisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/W12-2022.pdf}
}

@proceedings{bea-12,
  title = {Proceedings of the 7th Workshop on Innovative Use of {NLP} for Building
	Educational Applications ({BEA7}) at {NAACL-HLT}},
  year = {2012},
  editor = {Joel Tetreault and Jill Burstein and Claudial Leacock},
  address = {Montr√©al, Canada},
  publisher = {Association for Computational Linguistics},
  month = {June},
  booktitle = {Proceedings of the 7th Workshop on Innovative Use of {NLP} for Building
	Educational Applications ({BEA7}) at {NAACL-HLT}},
  comment = {cf. naacl-hlt-12},
  owner = {mzepf},
  timestamp = {2012.05.23}
}

