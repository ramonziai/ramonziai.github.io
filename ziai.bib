@comment{{This file has been generated by bib2bib 1.97}}

@comment{{Command line: /usr/bin/bib2bib -s year -r -oc ziai-citations -ob ziai.bib -c 'author : "Ziai"' -c 'year > 2008' /home/rziai/work/icall/resources/bibs/icall.bib}}

@misc{Wall.Ott.ea-14,
  title = {Macuna\'ima as a Data Source for the Distribution and Interpretation of Bare {NPs} in {B}razilian {P}ortuegese: Corpus preparation and first Annotation Agreement Results},
  author = {Albert Wall and Niels Ott and Ramon Ziai and Bj\"orn Rudzewitz},
  howpublished = {Poster at the Third International Conference on Ibero-Romance Historical Corpora (CODILI 3)},
  month = {6},
  year = {2014},
  address = {Zurich, Switzerland},
  owner = {nott},
  timestamp = {2014.06.27},
  url = {http://drni.de/zap/wall-ott-ea-14-poster}
}

@InProceedings{Rudzewitz.Ziai-15,
 author="Rudzewitz, Bj{\"o}rn and Ziai, Ramon",
 title="CoMiC: Adapting a Short Answer Assessment System for Answer Selection",
 booktitle="Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)",
 year="2015",
 publisher="Association for Computational Linguistics",
 pages="247--251",
 location="Denver, Colorado",
 url="http://aclweb.org/anthology/S15-2044"
}

@inproceedings{Ziai.Meurers-14,
  title = {Focus Annotation in Reading Comprehension Data},
  author = {Ramon Ziai and Detmar Meurers},
  booktitle = {Proceedings of the 8th {L}inguistic {A}nnotation {W}orkshop ({LAW VIII}, 2014)},
  year = {2014},
  pages = {159--168},
  address = {Dublin, Ireland},
  organization = {COLING},
  publisher = {Association for Computational Linguistics},
  url = {http://www.aclweb.org/anthology/W/W14/W14-4922.pdf}
}

@article{Higgins.Brew.ea-14,
  title = {Is getting the right answer just about choosing the right words?
 The role of syntactically-informed features in short answer scoring},
  author = {Derrick Higgins and Chris Brew and Michael Heilman and Ramon Ziai
 and Lei Chen and Aoife Cahill and Michael Flor and Nitin Madnani
 and Joel R. Tetreault and Daniel Blanchard and Diane Napolitano and
 Chong Min Lee and John Blackmore},
  journal = {Computation and Language},
  year = {2014},
  eprint = {1403.0801},
  eprinttype = {arxiv},
  file = {Higgins.Brew.ea-14.pdf:Higgins.Brew.ea-14.pdf:PDF},
  owner = {rziai},
  primaryclass = {cs.CL},
  timestamp = {2014.08.05},
  url = {http://arxiv.org/abs/1403.0801}
}

@inproceedings{Ott.Ziai.ea-13,
  title = {{CoMeT}: Integrating different levels of linguistic modeling for meaning assessment},
  author = {Niels Ott and Ramon Ziai and Michael Hahn and Detmar Meurers},
  booktitle = {Proceedings of the 7th International Workshop on Semantic Evaluation ({SemEval})},
  year = {2013},
  publisher = {Association for Computational Linguistics},
  address = {Atlanta, GA},
  pages = {608--616},
  file = {:Ott.Ziai.ea-13.pdf:PDF;S13-2102.pdf:Http\://aclweb.org/anthology/S13-2102.pdf:PDF},
  url = {http://aclweb.org/anthology/S13-2102.pdf}
}

@inproceedings{Ziai.Ott.ea-12a-no-crossref,
  title = {Short Answer Assessment: Establishing Links Between Research Strands},
  author = {Ramon Ziai and Niels Ott and Detmar Meurers},
  booktitle = {{Proceedings of the 7th Workshop on Innovative Use of {NLP} for {B}uilding {E}ducational {A}pplications (BEA-7) at {NAACL-HLT} 2012}},
  year = {2012},
  address = {Montreal},
  pages = {190--200},
  abstract = {A number of different research subfields are concerned with the automatic assessment of student answers to comprehension questions, from language learning contexts to computer science exams. They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others. This paper has the intention of fostering synergy between the different research strands. It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by Mohler et al. (2011) and outline what was necessary to perform this comparison. We conclude with a general discussion on comparability and evaluation of short answer assessment systems.},
  opteditor = {Joel Tetreault and Jill Burstein and Claudial Leacock},
  month = {June},
  publisher = {Association for Computational Linguistics},
  keywords = {rte recognizing textual entailment meaning comparison comic cosec automatic grading scoring weblas carmeltc c-rater oxford content assessment module cam bachman rose leacock chodorow mitchell pulman sukkarieh perez makatchev vanlehn bailey evaluation kappa gold standard kappa correlation},
  optpublisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/W12-2022.pdf}
}

@incollection{Ott.Ziai.Meurers-12,
  title = {Creation and Analysis of a Reading Comprehension Exercise Corpus: Towards Evaluating Meaning in Context},
  author = {Ott, Niels and Ziai, Ramon and Meurers, Detmar},
  booktitle = {Multilingual Corpora and Multilingual Corpus Analysis},
  publisher = {Benjamins},
  year = {2012},
  address = {Amsterdam},
  editor = {Thomas Schmidt and Kai W\"{o}rner},
  pages = {47--69},
  series = {Hamburg Studies in Multilingualism (HSM)},
  url = {http://purl.org/dm/papers/ott-ziai-meurers-12.html}
}

@article{Amaral.Meurers.Ziai-10,
  title = {Analyzing Learner Language: Towards A Flexible {NLP} Architecture for Intelligent Language Tutors},
  author = {Luiz Amaral and Detmar Meurers and Ramon Ziai},
  journal = {Computer-Assisted Language Learning},
  year = {2011},
  number = {1},
  pages = {1--16},
  volume = {24},
  file = {Amaral.Meurers.Ziai-10.pdf:Amaral.Meurers.Ziai-10.pdf:PDF},
  owner = {rziai},
  timestamp = {2010.04.09},
  url = {http://www.tandfonline.com/doi/pdf/10.1080/09588221.2010.520674}
}

@article{Meurers.Ziai.ea-11,
  title = {Integrating Parallel Analysis Modules to Evaluate the Meaning of Answers to Reading Comprehension Questions},
  author = {Detmar Meurers and Ramon Ziai and Niels Ott and Stacey Bailey},
  journal = {IJCEELL. Special Issue on Automatic Free-text Evaluation},
  year = {2011},
  number = {4},
  pages = {355--369},
  volume = {21},
  editor = {Jos\'{e} Antonio Le\'{o}n and Diana P\'{e}rez-Mar\'{i}n},
  publisher = {Inderscience Publishers},
  timestamp = {2010.11.24},
  url = {http://purl.org/dm/papers/meurers-ziai-ott-bailey-11.html}
}

@inproceedings{Meurers.Ziai.ea-11b,
  title = {Evaluating Answers to Reading Comprehension Questions in Context: Results for {German} and the Role of Information Structure},
  author = {Detmar Meurers and Ramon Ziai and Niels Ott and Janina Kopp},
  booktitle = {Proceedings of the {TextInfer} 2011 Workshop on Textual Entailment},
  year = {2011},
  address = {Edinburgh, Scotland, UK},
  month = {July},
  pages = {1--9},
  publisher = {Association for Computational Linguistics},
  file = {Meurers.Ziai.ea-11b.pdf:Meurers.Ziai.ea-11b.pdf:PDF},
  owner = {nott},
  timestamp = {2011.06.10},
  url = {http://aclweb.org/anthology/W11-2401}
}

@incollection{Widmann.Kohn.ea-11,
  title = {The {SACODEYL} search tool -- exploiting corpora for language learning purposes},
  author = {Johannes Widmann and Kurt Kohn and Ramon Ziai},
  booktitle = {New Trends in Corpora and Language Learning},
  publisher = {Continuum International Publishing Group},
  year = {2011},
  address = {London and New York},
  chapter = {10},
  editor = {Ana Frankenberg-Garcia and Lynne Flowerdew and Guy Aston},
  pages = {167--178},
  series = {Corpora and Discourse},
  volume = {14},
  owner = {rziai},
  timestamp = {2011.05.27}
}

@inproceedings{Meurers.Ott.ea-10,
  title = {Compiling a Task-Based Corpus for the Analysis of Learner Language in Context},
  author = {Detmar Meurers and Niels Ott and Ramon Ziai},
  booktitle = {Pre-Proceedings of Linguistic Evidence},
  year = {2010},
  address = {T\"{u}bingen},
  pages = {214--217},
  abstract = {Corpora in linguistics and computational linguistics have traditionally been assembled from data sources such as newspaper texts, books and, more recently, the web. While these sources provide large quantities of language data, typically very little or nothing is known about the context under which the text has been produced. The only information an analysis can refer to is the text itself, e.g., when a sentence is analyzed using the preceding sentences for disambiguation. However, language is always produced in a concrete extra-linguistic context. This contextual setting includes world knowledge and situational knowledge, i.e., the aspects of world knowledge which are relevant to interpret the given text and the concrete task and situation that the language was produced for. The notion of a task and the evaluation of language in context plays a particularly important role in foreign language teaching and learning (cf., e.g., Ellis 2003) and a representation of the learner's ability to use language in context and to perform tasks using appropriate task strategies has been argued to be crucial for learner modeling (Amaral and Meurers 2008). However, the so-called learner corpora created to document the language produced by language learners typically consist only of learner essays. In this paper, we present our efforts at collecting a longitudinal learner corpus consisting of the answers to reading comprehension questions, including an explicit representation of the task context and learner information. After introducing the data sources and characteristics of the corpus we are collecting, we discuss the development of the open-source WELCOME tool, which we have created to facilitate the interdisciplinary exchange of the contextualized learner corpus between the language programs providing the data and the computational linguists working on its encoding and automatic analysis.},
  file = {Meurers.Ott.ea-10.pdf:Meurers.Ott.ea-10.pdf:PDF},
  owner = {rziai},
  timestamp = {2010.02.25},
  url = {http://purl.org/dm/papers/meurers-ott-ziai-10.html}
}

@inproceedings{Meurers.Ziai.ea-10,
  title = {Enhancing Authentic Web Pages for Language Learners},
  author = {Detmar Meurers and Ramon Ziai and Luiz Amaral and Adriane Boyd and Aleksandar Dimitrov and Vanessa Metcalf and Niels Ott},
  booktitle = {{Proceedings of the 5th Workshop on Innovative Use of {NLP} for {B}uilding {E}ducational {A}pplications (BEA-5) at {NAACL-HLT} 2010}},
  year = {2010},
  address = {Los Angeles},
  pages = {10--18},
  publisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/W10-1002.pdf}
}

@inproceedings{Ott.Ziai-10,
  title = {Evaluating Dependency Parsing Performance on {G}erman Learner Language},
  author = {Niels Ott and Ramon Ziai},
  booktitle = {Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories},
  year = {2010},
  editor = {Markus Dickinson and Kaili M\"{u}\"{u}risep and Marco Passarotti},
  pages = {175--186},
  series = {NEALT Proceeding Series},
  volume = {9},
  file = {Ott.Ziai-10.pdf:Ott.Ziai-10.pdf:PDF},
  owner = {rziai},
  timestamp = {2010.10.18},
  url = {http://hdl.handle.net/10062/15960}
}

@mastersthesis{Ziai-09,
  title = {{A Flexible Annotation-Based Architecture for Intelligent Language Tutoring Systems}},
  author = {Ramon Ziai},
  school = {Universit\"{a}t T\"{u}bingen, Seminar f\"{u}r Sprachwissenschaft},
  year = {2009},
  month = {April},
  abstract = {This thesis presents a general architecture for Intelligent Language Tutoring Systems. The architecture makes use of annotation-based processing to encode different linguistic information at deep and shallow levels in a flexible manner and associate it with the learner input string. At the same time, it leaves the original input untouched, which is different from traditional pipeline approaches used in other Intelligent Computer-Assisted Language Learning systems. We show that this is necessary in order for feedback to be both error-specific and useful to the learner. Furthermore, the architecture incorporates a general mechanism for the use of activity models in guiding processing and feedback. The approach uses error types as a means of formulating pedagogical goals of individual activities. These error types are then mapped to analysis requirements that can be used to adapt processing of the learner input to the activity. More importantly, error types are used to influence the feedback strategy towards the errors that a particular activity centers on. We also demonstrate a similar use of error counts in the learner model to guide feedback. We have evaluated the system on the basis of example scenarios. They show that we are indeed able to give more useful feedback than the baseline system TAGARELA (Amaral, 2007) in a number of cases. This is the result of both annotation-based processing and integration of activity and learner model into the feedback process.},
  file = {Ziai-09.pdf:Ziai-09.pdf:PDF},
  owner = {rziai},
  timestamp = {2009.07.23},
  url = {http://www.sfs.uni-tuebingen.de/~rziai/papers/Ziai-09.pdf}
}

